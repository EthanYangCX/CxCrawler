{
 "cells": [
  {
   "source": [
    "# data to xlsx func\n",
    "\n",
    "目前 QS、THE 所用之 json 大同小异，吾亦将 ARWU 数据爬取为彼样 json，故使用统一函数处理之。json 如：\n",
    "\n",
    "```json\n",
    "{\"data\": [{\"name\": \"TUH\", \"rank\": \"0\", \"score\": \"10422\"}]}\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a common function to process json gotten by crawler\n",
    "import requests\n",
    "import json\n",
    "import xlsxwriter as xlwt\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def ranker_to_col_names(ranker):\n",
    "    col_name_dic = {'name': 'name', 'rank': 'rank', 'score': 'score'} # , 'region': 'region', 'aliases': 'aliases'\n",
    "    if ranker == 'QS':\n",
    "        col_name_dic['name'] = 'title'\n",
    "        col_name_dic['rank'] = 'rank_display'\n",
    "        col_name_dic['score'] = 'score'\n",
    "        col_name_dic['region'] = 'region'\n",
    "    elif ranker == 'THE':\n",
    "        col_name_dic['name'] = 'name'\n",
    "        col_name_dic['rank'] = 'rank'\n",
    "        col_name_dic['score'] = 'scores_overall'\n",
    "        col_name_dic['region'] = 'location'\n",
    "        col_name_dic['aliases'] = 'aliases'\n",
    "    elif ranker == 'ARWU':\n",
    "        col_name_dic = col_name_dic\n",
    "    return col_name_dic\n",
    "\n",
    "# 将字典数据转换为数组\n",
    "def parser_page(json, ranker):\n",
    "    col_name_dic = ranker_to_col_names(ranker)\n",
    "    if json:\n",
    "        items = json.get('data')\n",
    "        for i in range(len(items)):\n",
    "            item = items[i]\n",
    "            res = {}\n",
    "            for col in col_name_dic.keys():\n",
    "                if col == 'name':\n",
    "                    res['name'] = BeautifulSoup(item[col_name_dic['name']]).get_text()\n",
    "                elif col == 'rank':\n",
    "                    if \"=\" in item[col_name_dic['rank']]:\n",
    "                        rk_str = str(item[col_name_dic['rank']]).split('=')[-1]\n",
    "                        res['rank'] = rk_str\n",
    "                    else:\n",
    "                        res['rank'] = item[col_name_dic['rank']]\n",
    "                else:\n",
    "                    res[col] = item[col_name_dic[col]]\n",
    "            yield res\n",
    "\n",
    "def load_data_to_xlsx(ranker, years):\n",
    "    workbook = xlwt.Workbook('{0}.xlsx'.format(ranker))\n",
    "    worksheet_main = workbook.add_worksheet('main')\n",
    "\n",
    "    col_name_dic = ranker_to_col_names(ranker)\n",
    "\n",
    "    # 写每一年的分表\n",
    "    for year in years:\n",
    "        with open('{0}-json\\\\{1}.json'.format(ranker, year), 'r', encoding='utf-8') as f:\n",
    "            data = json.loads(f.read())\n",
    "        results = parser_page(data, ranker)\n",
    "\n",
    "        worksheet = workbook.add_worksheet(year)\n",
    "\n",
    "        headings = list(col_name_dic.keys())\n",
    "        col = 0\n",
    "        for heading in headings:\n",
    "            worksheet.write(0, col, heading)\n",
    "            col = col+1\n",
    "\n",
    "        row = 1\n",
    "        for result in results:\n",
    "            col = 0\n",
    "            for k, v in result.items():\n",
    "                worksheet.write(row, col, v)\n",
    "                col = col + 1\n",
    "            row = row + 1\n",
    "\n",
    "    # 写 main 表\n",
    "    with open('{0}-json\\\\{1}.json'.format(ranker, years[0]), 'r', encoding='utf-8') as f:\n",
    "        data = json.loads(f.read())\n",
    "    results = parser_page(data, ranker)\n",
    "    headings_main = ['University Name'] + years\n",
    "    col = 0\n",
    "    for heading in headings_main:\n",
    "        worksheet_main.write(0, col, heading)\n",
    "        col += 1\n",
    "    row = 1\n",
    "    for result in results:\n",
    "        worksheet_main.write(row, 0, result['name'])\n",
    "        for year in years:\n",
    "            worksheet_main.write(\n",
    "                row, headings_main.index(year), \"=VLOOKUP(A{0},'{1}'!A:B,2,0)\".format(row + 1, year))\n",
    "        row += 1\n",
    "\n",
    "    worksheet_main.set_column('A:A',20) \n",
    "    workbook.close()"
   ]
  },
  {
   "source": [
    "# QS to grab\n",
    "using XHR gotten in urls like https://www.topuniversities.com/university-rankings/world-university-rankings/2022"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从network的xhr文件中发现json文件的url\n",
    "urls={\n",
    "    '2022': 'https://www.topuniversities.com/sites/default/files/qs-rankings-data/en/3740566.txt?1623183713?v=1623251204159',\n",
    "    '2021': 'https://www.topuniversities.com/sites/default/files/qs-rankings-data/en/2057712.txt?1623183364?v=1623254149518',\n",
    "    '2020': 'https://www.topuniversities.com/sites/default/files/qs-rankings-data/en/914824.txt?1623183839?v=1623254203592',\n",
    "    '2019': 'https://www.topuniversities.com/sites/default/files/qs-rankings-data/en/397863.txt?1623183911?v=1623254242977',\n",
    "    '2018': 'https://www.topuniversities.com/sites/default/files/qs-rankings-data/en/357051.txt?1623189642?v=1623254394658',\n",
    "    '2017': 'https://www.topuniversities.com/sites/default/files/qs-rankings-data/en/357051.txt?1623189642?v=1623254422154',\n",
    "    '2016': 'https://www.topuniversities.com/sites/default/files/qs-rankings-data/en/326584.txt?1623203022?v=1623254459114',\n",
    "    '2015': 'https://www.topuniversities.com/sites/default/files/qs-rankings-data/en/299926.txt?1623193600?v=1623254527088'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crawl\n",
    "import requests\n",
    "import json\n",
    "for year, url in urls.items():\n",
    "    cont = requests.get(url)\n",
    "    data = cont.json()   # json数据类型为dict\n",
    "    with open(r'.\\QS-json\\{0}.json'.format(year), 'w', encoding='utf-8') as f:\n",
    "        f.write(json.dumps(data))"
   ]
  },
  {
   "source": [
    "# QS to analyse\n",
    "process data collected from qs website"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_data_to_xlsx('QS', [str(i) for i in range(2022, 2015 - 1, -1)])"
   ]
  },
  {
   "source": [
    "# THE data getting\n",
    "like https://www.timeshighereducation.com/world-university-rankings/2021/world-ranking\n",
    "THE 的网页和QS一样都把数据存在 XHR 里，所以找到对应的 json 文件链接便可获取完整数据。但要额外注意的是 THE 的网站对 headers 有检查，不加 headers 会返回 403。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从network的xhr文件中发现json文件的url\n",
    "urls={\n",
    "    '2021': 'https://www.timeshighereducation.com/sites/default/files/the_data_rankings/world_university_rankings_2021_0__fa224219a267a5b9c4287386a97c70ea.json',\n",
    "    '2020': 'https://www.timeshighereducation.com/sites/default/files/the_data_rankings/world_university_rankings_2020_0__24cc3874b05eea134ee2716dbf93f11a.json',\n",
    "    '2019': 'https://www.timeshighereducation.com/sites/default/files/the_data_rankings/world_university_rankings_2019_0__8923a34186e552aa8aec863e45bc02d5.json',\n",
    "    '2018': 'https://www.timeshighereducation.com/sites/default/files/the_data_rankings/world_university_rankings_2018_0__e814f039fcc8ddc45dc6085e4a8a8b66.json',\n",
    "    '2017': 'https://www.timeshighereducation.com/sites/default/files/the_data_rankings/world_university_rankings_2017_0__06ec07a66faf58bb6171791e5852fe1c.json',\n",
    "    '2016': 'https://www.timeshighereducation.com/sites/default/files/the_data_rankings/world_university_rankings_2016_0__db6aad7f77c771c83817ced2b2ed3722.json',\n",
    "    '2015': 'https://www.timeshighereducation.com/sites/default/files/the_data_rankings/world_university_rankings_2015_0__0ea44da985e26d09045368c98f819b91.json'\n",
    "}\n",
    "latest_year = list(urls.keys())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "# crawl\n",
    "import requests\n",
    "import json\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.99 Safari/537.36'}\n",
    "for year, url in urls.items():\n",
    "    cont = requests.get(url, headers=headers)\n",
    "    print(cont)\n",
    "    data = cont.json()   # json数据类型为dict\n",
    "    with open(r'.\\THE-json\\{0}.json'.format(year), 'w', encoding='utf-8') as f:\n",
    "        f.write(json.dumps(data))"
   ]
  },
  {
   "source": [
    "# THE to analyse"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_data_to_xlsx('THE', [str(i) for i in range(2021, 2015 - 1, -1)])"
   ]
  },
  {
   "source": [
    "# ARWU data getting\n",
    "\n",
    "it appears to use js to display data, not XHR. so I use selenium to click and get data per page.\n",
    "\n",
    "**TODO: solve it that some univs in this ranking has different names in different years, e. g. ETH, ICL, Berkeley**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import Select\n",
    "\n",
    "option = Options()\n",
    "option.add_argument(\"--disable-notifications\")\n",
    "\n",
    "# find limited page number\n",
    "def get_final_page_number(url):\n",
    "    url = url\n",
    "    chrome = webdriver.Chrome('./chromedriver', options=option)\n",
    "    chrome.get(url)\n",
    "\n",
    "    soup = BeautifulSoup(chrome.page_source, 'lxml')\n",
    "    flag = False\n",
    "    tags_a = soup.find_all(\"a\")\n",
    "    limited_page = 0\n",
    "    for tag in tags_a:\n",
    "        if(tag.text == \"•••\" and flag == False):\n",
    "            flag = True\n",
    "        elif(flag == True):\n",
    "            try:\n",
    "                limited_page = int(tag.text)\n",
    "            except:\n",
    "                print(\"ERROR: Error with convert page number to integer.\")\n",
    "            break\n",
    "    print(\"limit page:\", limited_page)\n",
    "    chrome.close()\n",
    "    return limited_page\n",
    "\n",
    "# get one page's infomation\n",
    "def get_page_info(soup):\n",
    "    items = soup.find_all(\"tr\")\n",
    "    flag = False\n",
    "    data = []\n",
    "    cols = ['rank', 'score', 'name']\n",
    "\n",
    "    for row in items:\n",
    "        if(flag == False):\n",
    "            flag = True\n",
    "            continue\n",
    "        single_data = {}\n",
    "        # score and ranking\n",
    "        try:\n",
    "            text = row.contents[0].text.lstrip().rstrip()\n",
    "            single_data['rank'] = text\n",
    "            text = row.contents[4].text.lstrip().rstrip()\n",
    "            single_data['score'] = text\n",
    "        except:\n",
    "            print(\"ERROR: score ranking error.\")\n",
    "            pass\n",
    "        # institution name\n",
    "        try:\n",
    "            text = row.find(class_=\"univ-name\").text.lstrip().rstrip()\n",
    "            single_data['name'] = text\n",
    "        except:\n",
    "            try:\n",
    "                text = row.find(class_=\"univ-name-normal\").text.lstrip().rstrip()\n",
    "                single_data['name'] = text\n",
    "            except:\n",
    "                print(\"ERROR: university name error.\")\n",
    "                pass\n",
    "        \n",
    "        if(len(single_data) == 3):\n",
    "            data.append(single_data)\n",
    "        else:\n",
    "            print(single_data, \" row error.\")\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "limit page: 17\n",
      "[Page:  1] [Page:  2] [Page:  3] [Page:  4] [Page:  5] [Page:  6] [Page:  7] [Page:  8] [Page:  9] [Page: 10] [Page: 11] [Page: 12] [Page: 13] [Page: 14] [Page: 15] [Page: 16] [Page: 17] - finish -\n",
      "limit page: 17\n",
      "[Page:  1] [Page:  2] [Page:  3] [Page:  4] [Page:  5] [Page:  6] [Page:  7] [Page:  8] [Page:  9] [Page: 10] [Page: 11] [Page: 12] [Page: 13] [Page: 14] [Page: 15] [Page: 16] [Page: 17] - finish -\n",
      "limit page: 17\n",
      "[Page:  1] [Page:  2] [Page:  3] [Page:  4] [Page:  5] [Page:  6] [Page:  7] [Page:  8] [Page:  9] [Page: 10] [Page: 11] [Page: 12] [Page: 13] [Page: 14] [Page: 15] [Page: 16] [Page: 17] - finish -\n",
      "limit page: 17\n",
      "[Page:  1] [Page:  2] [Page:  3] [Page:  4] [Page:  5] [Page:  6] [Page:  7] [Page:  8] [Page:  9] [Page: 10] [Page: 11] [Page: 12] [Page: 13] [Page: 14] [Page: 15] [Page: 16] [Page: 17] - finish -\n",
      "limit page: 17\n",
      "[Page:  1] [Page:  2] [Page:  3] [Page:  4] [Page:  5] [Page:  6] [Page:  7] [Page:  8] [Page:  9] [Page: 10] [Page: 11] [Page: 12] [Page: 13] [Page: 14] [Page: 15] [Page: 16] [Page: 17] - finish -\n",
      "limit page: 17\n",
      "[Page:  1] [Page:  2] [Page:  3] [Page:  4] [Page:  5] [Page:  6] [Page:  7] [Page:  8] [Page:  9] [Page: 10] [Page: 11] [Page: 12] [Page: 13] [Page: 14] [Page: 15] [Page: 16] [Page: 17] - finish -\n",
      "limit page: 17\n",
      "[Page:  1] [Page:  2] [Page:  3] [Page:  4] [Page:  5] [Page:  6] [Page:  7] [Page:  8] [Page:  9] [Page: 10] [Page: 11] [Page: 12] [Page: 13] [Page: 14] [Page: 15] [Page: 16] [Page: 17] - finish -\n",
      "limit page: 27\n",
      "[Page:  1] [Page:  2] [Page:  3] [Page:  4] [Page:  5] [Page:  6] [Page:  7] [Page:  8] [Page:  9] [Page: 10] [Page: 11] [Page: 12] [Page: 13] [Page: 14] [Page: 15] [Page: 16] [Page: 17] [Page: 18] [Page: 19] [Page: 20] [Page: 21] [Page: 22] [Page: 23] [Page: 24] [Page: 25] [Page: 26] [Page: 27] - finish -\n",
      "limit page: 34\n",
      "[Page:  1] [Page:  2] [Page:  3] [Page:  4] [Page:  5] [Page:  6] [Page:  7] [Page:  8] [Page:  9] [Page: 10] [Page: 11] [Page: 12] [Page: 13] [Page: 14] [Page: 15] [Page: 16] [Page: 17] [Page: 18] [Page: 19] [Page: 20] [Page: 21] [Page: 22] [Page: 23] [Page: 24] [Page: 25] [Page: 26] [Page: 27] [Page: 28] [Page: 29] [Page: 30] [Page: 31] [Page: 32] [Page: 33] [Page: 34] - finish -\n",
      "limit page: 34\n",
      "[Page:  1] [Page:  2] [Page:  3] [Page:  4] [Page:  5] [Page:  6] [Page:  7] [Page:  8] [Page:  9] [Page: 10] [Page: 11] [Page: 12] [Page: 13] [Page: 14] [Page: 15] [Page: 16] [Page: 17] [Page: 18] [Page: 19] [Page: 20] [Page: 21] [Page: 22] [Page: 23] [Page: 24] [Page: 25] [Page: 26] [Page: 27] [Page: 28] [Page: 29] [Page: 30] [Page: 31] [Page: 32] [Page: 33] [Page: 34] - finish -\n",
      "limit page: 34\n",
      "[Page:  1] [Page:  2] [Page:  3] [Page:  4] [Page:  5] [Page:  6] [Page:  7] [Page:  8] [Page:  9] [Page: 10] [Page: 11] [Page: 12] [Page: 13] [Page: 14] [Page: 15] [Page: 16] [Page: 17] [Page: 18] [Page: 19] [Page: 20] [Page: 21] [Page: 22] [Page: 23] [Page: 24] [Page: 25] [Page: 26] [Page: 27] [Page: 28] [Page: 29] [Page: 30] [Page: 31] [Page: 32] [Page: 33] [Page: 34] - finish -\n"
     ]
    }
   ],
   "source": [
    "# crawler\n",
    "for year in range(2010, 2020 + 1):\n",
    "    url = \"http://www.shanghairanking.com/rankings/arwu/{}\".format(year)\n",
    "    chrome = webdriver.Chrome('./chromedriver', options=option)\n",
    "    chrome.get(url)\n",
    "\n",
    "    count = 0\n",
    "    limited_page = get_final_page_number(url)\n",
    "    data = {'data': []}\n",
    "\n",
    "    while(count < limited_page):\n",
    "        time.sleep(1)\n",
    "        print(\"[Page: %2d] \" % (count + 1), end=\"\")\n",
    "\n",
    "        soup = BeautifulSoup(chrome.page_source, 'lxml')\n",
    "        data['data'] += get_page_info(soup)\n",
    "        # start: construct all lists' info\n",
    "        \n",
    "        time.sleep(2)\n",
    "\n",
    "        # click next page\n",
    "        btns_next_page = chrome.find_elements_by_class_name(\"ant-pagination-item-link\", )\n",
    "        btns_next_page[-1].click()\n",
    "        count += 1\n",
    "\n",
    "    chrome.close()\n",
    "    print(\"- finish -\")\n",
    "    with open(r'.\\ARWU-json\\{0}.json'.format(year), 'w', encoding='utf-8') as f:\n",
    "        f.write(json.dumps(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_data_to_xlsx('ARWU', [str(i) for i in range(2020, 2010 - 1, -1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python376jvsc74a57bd0b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}